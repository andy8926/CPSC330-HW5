{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw5.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 5: Putting it all together \n",
    "### Associated lectures: All material till lecture 13 \n",
    "\n",
    "**Due date: [Monday, Mar 10, 11:59 pm](https://github.com/UBC-CS/cpsc330-2024W2?tab=readme-ov-file#deliverable-due-dates-tentative)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "0. [Submission instructions](#si)\n",
    "1. [Understanding the problem](#1)\n",
    "2. [Data splitting](#2)\n",
    "3. [EDA](#3)\n",
    "4. [Feature engineering](#4)\n",
    "5. [Preprocessing and transformations](#5) \n",
    "6. [Baseline model](#6)\n",
    "7. [Linear models](#7)\n",
    "8. [Different models](#8)\n",
    "9. [Feature selection](#9)\n",
    "10. [Hyperparameter optimization](#10)\n",
    "11. [Interpretation and feature importances](#11) \n",
    "12. [Results on the test set](#12)\n",
    "13. [Summary of the results](#13)\n",
    "14. [Your takeaway from the course](#15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## Submission instructions\n",
    "<hr>\n",
    "rubric={points:4}\n",
    "\n",
    "**You may work with a partner on this homework and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 2. \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
    "- If you would like to use late tokens for the homework, all group members must have the necessary late tokens available. Please note that the late tokens will be counted for all members of the group.   \n",
    "\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330-2024W2/blob/master/docs/homework_instructions.md). \n",
    "\n",
    "1. Before submitting the assignment, run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from \"1\" will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Follow the [CPSC 330 homework instructions](https://ubc-cs.github.io/cpsc330-2024W2/docs/homework_instructions.html), which include information on how to do your assignment and how to submit your assignment.\n",
    "4. Upload your solution on Gradescope. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission. \n",
    "5. Make sure that the plots and output are rendered properly in your submitted file. If the .ipynb file is too big and doesn't render on Gradescope, also upload a pdf or html in addition to the .ipynb so that the TAs can view your submission on Gradescope.\n",
    "\n",
    "\n",
    "_Note: The assignments will get gradually more open-ended as we progress through the course. In many cases, there won't be a single correct solution. Sometimes you will have to make your own choices and your own decisions (for example, on what parameter values to use when they are not explicitly provided in the instructions). Use your own judgment in such cases and justify your choices, if necessary._\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Imports\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import sys\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(\"..\"), \"code\"))\n",
    "import seaborn as sns\n",
    "# from plotting_functions import *\n",
    "from sklearn import datasets\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "# from utils import *\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress Warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*frozen modules.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*'force_all_finite' was renamed.*\")\n",
    "warnings.simplefilter(\"ignore\")\n",
    "warnings.simplefilter(action=\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Introduction <a name=\"in\"></a>\n",
    "\n",
    "In this homework you will be working on an open-ended mini-project, where you will put all the different things you have learned so far together to solve an interesting problem.\n",
    "\n",
    "A few notes and tips when you work on this mini-project: \n",
    "\n",
    "#### Tips\n",
    "1. This mini-project is open-ended, and while working on it, there might be some situations where you'll have to use your own judgment and make your own decisions (as you would be doing when you work as a data scientist). Make sure you explain your decisions whenever necessary. \n",
    "2. **Do not include everything you ever tried in your submission** -- it's fine just to have your final code. That said, your code should be reproducible and well-documented. For example, if you chose your hyperparameters based on some hyperparameter optimization experiment, you should leave in the code for that experiment so that someone else could re-run it and obtain the same hyperparameters, rather than mysteriously just setting the hyperparameters to some (carefully chosen) values in your code. \n",
    "3. If you realize that you are repeating a lot of code try to organize it in functions. Clear presentation of your code, experiments, and results is the key to be successful in this lab. You may use code from lecture notes or previous lab solutions with appropriate attributions. \n",
    "\n",
    "#### Assessment\n",
    "We plan to grade fairly and leniently. We don't have some secret target score that you need to achieve to get a good grade. **You'll be assessed on demonstration of mastery of course topics, clear presentation, and the quality of your analysis and results.** For example, if you just have a bunch of code and no text or figures, that's not good. If you do a bunch of sane things and get a lower accuracy than your friend, don't sweat it.\n",
    "\n",
    "\n",
    "#### A final note\n",
    "Finally, this style of this \"project\" question is different from other assignments. It'll be up to you to decide when you're \"done\" -- in fact, this is one of the hardest parts of real projects. But please don't spend WAY too much time on this... perhaps \"a few hours\" (15-20 hours???) is a good guideline for this project . Of course if you're having fun you're welcome to spend as much time as you want! But, if so, try not to do it out of perfectionism or getting the best possible grade. Do it because you're learning and enjoying it. Students from the past cohorts have found such kind of labs useful and fun and I hope you enjoy it as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 1. Pick your problem and explain the prediction problem <a name=\"1\"></a>\n",
    "<hr>\n",
    "rubric={points:3}\n",
    "\n",
    "In this mini project, you have the option to choose on which dataset you will be working on. The tasks you will need to carry on will be similar, independently of your choice.\n",
    "\n",
    "### Option 1\n",
    "You can choose to work on a classification problem of predicting whether a credit card client will default or not. \n",
    "For this problem, you will use [Default of Credit Card Clients Dataset](https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset). In this data set, there are 30,000 examples and 24 features, and the goal is to estimate whether a person will default (fail to pay) their credit card bills; this column is labeled \"default.payment.next.month\" in the data. The rest of the columns can be used as features. You may take some ideas and compare your results with [the associated research paper](https://www.sciencedirect.com/science/article/pii/S0957417407006719), which is available through [the UBC library](https://www.library.ubc.ca/). \n",
    "\n",
    "\n",
    "### Option 2\n",
    "You can choose to work on a regression problem using a [dataset](https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data) of New York City Airbnb listings from 2019. As usual, you'll need to start by downloading the dataset, then you will try to predict `reviews_per_month`, as a proxy for the popularity of the listing. Airbnb could use this sort of model to predict how popular future listings might be before they are posted, perhaps to help guide hosts create more appealing listings. In reality they might instead use something like vacancy rate or average rating as their target, but we do not have that available here.\n",
    "\n",
    "> Note there is an updated version of this dataset with more features available [here](http://insideairbnb.com/). The features were are using in `listings.csv.gz` for the New York city datasets. You will also see some other files like `reviews.csv.gz`. For your own interest you may want to explore the expanded dataset and try your analysis there. However, please submit your results on the dataset obtained from Kaggle.\n",
    "\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Spend some time understanding the options and pick the one you find more interesting (it may help spending some time looking at the documentation available on Kaggle for each dataset).\n",
    "2. After making your choice, focus on understanding the problem and what each feature means, again using the documentation on the dataset page on Kaggle. Write a few sentences on your initial thoughts on the problem and the dataset. \n",
    "3. Download the dataset and read it as a pandas dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "airbnb_df = pd.read_csv(\"data/AB_NYC_2019.csv\")\n",
    "airbnb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 2. Data splitting <a name=\"2\"></a>\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Split the data into train (70%) and test (30%) portions with `random_state=123`.\n",
    "\n",
    "> If your computer cannot handle training on 70% training data, make the test split bigger.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(airbnb_df, test_size=0.30, random_state=123)\n",
    "\n",
    "X_train, y_train = (\n",
    "    train_df.drop(columns=[\"reviews_per_month\"]),\n",
    "    train_df[\"reviews_per_month\"],\n",
    ")\n",
    "\n",
    "X_test, y_test = (\n",
    "    test_df.drop(columns=[\"reviews_per_month\"]),\n",
    "    test_df[\"reviews_per_month\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 3. EDA <a name=\"3\"></a>\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Perform exploratory data analysis on the train set.\n",
    "2. Include at least two summary statistics and two visualizations that you find useful, and accompany each one with a sentence explaining it.\n",
    "3. Summarize your initial observations about the data. \n",
    "4. Pick appropriate metric/metrics for assessment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following columns have some missing values:\n",
    "  - name\n",
    "  - host_name\n",
    "  - last_review\n",
    "  - reviews_per_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reviews_per_month, it might be reasonable to fill Nah in with 0\n",
    "\n",
    "y_train = y_train.fillna(0)\n",
    "y_test = y_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.hist(bins=50, figsize=(20, 15));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Since reviews_per_month is a continuous variable, this is a regression problem. \n",
    "Possible metrics would be:\n",
    "- RMSE\n",
    "- R^2\n",
    "- MAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here we can see that the reviews_per_month, price, number_of_reviews, calculated_host_listings_cost are skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common trick in such cases is applying a log transform on the target column to make it more normal and less skewed.\n",
    "That is, transform \n",
    ".\n",
    "Linear regression will usually work better on something that looks more normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 4. Feature engineering <a name=\"4\"></a>\n",
    "<hr>\n",
    "rubric={points:1}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Carry out feature engineering. In other words, extract new features relevant for the problem and work with your new feature set in the following exercises. You may have to go back and forth between feature engineering and preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group neighbourhood_group into categories\n",
    "def categorize_neighbourhood_group(group):\n",
    "    if group == \"Manhattan\":\n",
    "        return 'Manhattan'\n",
    "    elif group == \"Brooklyn\":\n",
    "        return 'Brooklyn'\n",
    "    else:\n",
    "        return \"Others\"\n",
    "\n",
    "X_train['neighbourhood_group'] = X_train['neighbourhood_group'].apply(categorize_neighbourhood_group)\n",
    "X_test['neighbourhood_group'] = X_test['neighbourhood_group'].apply(categorize_neighbourhood_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group room_type into categories\n",
    "def categorize_room_type(room_type):\n",
    "    if room_type == \"Entire home/apt\":\n",
    "        return 'Entire home/apt'\n",
    "    elif room_type == \"Private room\":\n",
    "        return 'Private room'\n",
    "    else:\n",
    "        return \"Others\"\n",
    "\n",
    "X_train['room_type'] = X_train['room_type'].apply(categorize_room_type)\n",
    "X_test['room_type'] = X_test['room_type'].apply(categorize_room_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group minimum_nights into categories\n",
    "def categorize_min_nights(nights):\n",
    "    if nights <= 1:\n",
    "        return 'Short Stay'\n",
    "    elif nights <= 3:\n",
    "        return 'Medium_Short Stay'\n",
    "    elif nights <= 5:\n",
    "        return 'Medium_Long Stay'\n",
    "    else:\n",
    "        return \"Long Stay\"\n",
    "\n",
    "X_train['minimum_nights'] = X_train['minimum_nights'].apply(categorize_min_nights)\n",
    "X_test['minimum_nights'] = X_test['minimum_nights'].apply(categorize_min_nights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group calculated_host_listings_count into categories\n",
    "# amount of listing per host\n",
    "\n",
    "def categorize_host_listings(listing_count):\n",
    "    if listing_count <= 10:\n",
    "        return 'Little or no listings'\n",
    "    else:\n",
    "        return \"Many listings\"\n",
    "\n",
    "X_train['calculated_host_listings_count'] = X_train['calculated_host_listings_count'].apply(categorize_host_listings)\n",
    "X_test['calculated_host_listings_count'] = X_test['calculated_host_listings_count'].apply(categorize_host_listings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 5. Preprocessing and transformations <a name=\"5\"></a>\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Identify different feature types and the transformations you would apply on each feature type. \n",
    "2. Define a column transformer, if necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normal_numeric_feats = ['latitude','longitude']\n",
    "\n",
    "skewed_numeric_feats = ['availability_365',\n",
    "                 'number_of_reviews', 'price']\n",
    "\n",
    "categorical_feats = ['minimum_nights','neighbourhood_group','neighbourhood','room_type','calculated_host_listings_count'] \n",
    " # apply one-hot encoding # room type ordinal???\n",
    "\n",
    "drop_feats = [\n",
    "    'id',\n",
    "    'name',\n",
    "    'host_id',\n",
    "    'host_name',\n",
    "    'last_review',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transformer to apply log1p\n",
    "from sklearn.pipeline import FunctionTransformer\n",
    "\n",
    "log_transformer = FunctionTransformer(np.log1p, feature_names_out='one-to-one')\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    # 1. Right-skewed numeric features → impute → log1p → scale\n",
    "    (make_pipeline(SimpleImputer(), log_transformer, StandardScaler()), skewed_numeric_feats),\n",
    "\n",
    "    # 2. Normally distributed numeric features → impute → scale\n",
    "    (make_pipeline(SimpleImputer(), StandardScaler()), normal_numeric_feats),\n",
    "\n",
    "    # 3. Categorical features → one-hot encode\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), categorical_feats),\n",
    "\n",
    "    # 4. Drop unwanted features\n",
    "    (\"drop\", drop_feats)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 6. Baseline model <a name=\"6\"></a>\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Try `scikit-learn`'s baseline model and report results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_6\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_dict = {}  # dictionary to store all the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model :\n",
    "        scikit-learn model\n",
    "    X_train : numpy array or pandas DataFrame\n",
    "        X in the training data\n",
    "    y_train :\n",
    "        y in the training data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        pandas Series with mean scores from cross_validation\n",
    "    \"\"\"\n",
    "\n",
    "    y_train_log = np.log1p(y_train)  # log(1 + y), avoids issues with 0\n",
    "\n",
    "    scores = cross_validate(model, X_train, y_train_log, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores.iloc[i], std_scores.iloc[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "dummy = DummyRegressor()\n",
    "pipe = make_pipeline(preprocessor, dummy)\n",
    "results_dict[\"dummy\"] = mean_std_cross_val_scores(pipe, X_train, y_train, cv=5,return_train_score=True)\n",
    "results_df = pd.DataFrame(results_dict).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I evaluated the DummyRegressor using R² as the scoring metric and obtained a result of approximately 0.000 on the training and the test(cv) set. This outcome is expected, as the Dummy Regressor simply predicts a constant value, which captures no variance in the data.\n",
    "\n",
    "Since R² measures how well the model explains the variance in the target, a result close to zero indicates that the model performs no better than a naive average guess. However, in this context, the target variable reviews_per_month contains a large number of zeros and has a skewed distribution. That makes R² less informative, especially when many values are clustered around zero and there’s little variance to explain.\n",
    "\n",
    "Therefore, a metric like RMSE or MAPE might be more appropriate, as it gives a better sense of relative error and can handle skewed data more meaningfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 7. Linear models <a name=\"7\"></a>\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Try a linear model as a first real attempt. \n",
    "2. Carry out hyperparameter tuning to explore different values for the complexity hyperparameter. \n",
    "3. Report cross-validation scores along with standard deviation. \n",
    "4. Summarize your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_7\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ridge_pipe = make_pipeline(preprocessor, Ridge(random_state=123))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = {\"ridge__alpha\": 10.0 ** np.arange(-6, 6, 1),\n",
    "              \"ridge__max_iter\": [10,15,20,25,30]}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    ridge_pipe, param_grid, n_iter=12, return_train_score=True, n_jobs=-1, verbose=0,random_state=123)\n",
    "\n",
    "y_train_log = np.log1p(y_train)  # log(1 + y), avoids issues with 0\n",
    "\n",
    "# Fit the search object\n",
    "search.fit(X_train, y_train_log)\n",
    "\n",
    "# Print the best hyperparameters and score\n",
    "print(\"Best hyperparameter values: \", search.best_params_)\n",
    "print(\"Best score: %0.3f\" % (search.best_score_))\n",
    "\n",
    "# Convert the search results to a DataFrame and display\n",
    "pd.DataFrame(search.cv_results_)[\n",
    "    [\n",
    "        \"mean_train_score\",\n",
    "        \"mean_test_score\",\n",
    "        \"param_ridge__alpha\",\n",
    "        \"param_ridge__max_iter\",\n",
    "        \"mean_fit_time\",\n",
    "        \"rank_test_score\",\n",
    "    ]\n",
    "].set_index(\"rank_test_score\").sort_index().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_pipe_tuned = make_pipeline(preprocessor, Ridge(alpha=10,max_iter=30, random_state=123))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_dict[\"ridge\"] = mean_std_cross_val_scores(\n",
    "    ridge_pipe_tuned, X_train, y_train, cv=5,return_train_score=True\n",
    ")\n",
    "results_df = pd.DataFrame(results_dict).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 8. Different models <a name=\"8\"></a>\n",
    "<hr>\n",
    "rubric={points:12}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Try at least 3 other models aside from a linear model. One of these models should be a tree-based ensemble model. \n",
    "2. Summarize your results in terms of overfitting/underfitting and fit and score times. Can you beat a linear model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_8\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "pipe_lgbm = make_pipeline(\n",
    "    preprocessor, LGBMRegressor(verbosity=-1, random_state=123, n_jobs=-1)\n",
    ")\n",
    "results_dict[\"LGBM\"] = mean_std_cross_val_scores(\n",
    "    pipe_lgbm, X_train, y_train, cv=5, return_train_score=True\n",
    ")\n",
    "results_df = pd.DataFrame(results_dict).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "pipe_xgb = make_pipeline(preprocessor, XGBRegressor(n_jobs=-1,random_state=123))\n",
    "results_dict[\"xgboost\"] = mean_std_cross_val_scores(\n",
    "    pipe_xgb, X_train, y_train, cv=5, return_train_score=True\n",
    ")\n",
    "results_df = pd.DataFrame(results_dict).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forest_pipe = make_pipeline(preprocessor, RandomForestRegressor(n_jobs=-1, max_features=26,random_state=123))\n",
    "results_dict[\"random_forest\"] = mean_std_cross_val_scores(\n",
    "    forest_pipe, X_train, y_train, cv=5, return_train_score=True\n",
    ")\n",
    "results_df = pd.DataFrame(results_dict).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 9. Feature selection <a name=\"9\"></a>\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Make some attempts to select relevant features. You may try `RFECV` or forward selection for this. Do the results improve with feature selection? Summarize your results. If you see improvements in the results, keep feature selection in your pipeline. If not, you may abandon it in the next exercises. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_9\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Feature  Importance\n",
      "1                  pipeline-1__number_of_reviews         672\n",
      "0                   pipeline-1__availability_365         586\n",
      "4                          pipeline-2__longitude         380\n",
      "3                           pipeline-2__latitude         356\n",
      "2                              pipeline-1__price         313\n",
      "..                                           ...         ...\n",
      "48      onehotencoder__neighbourhood_South Slope           1\n",
      "44        onehotencoder__neighbourhood_Rego Park           1\n",
      "24        onehotencoder__neighbourhood_Flatlands           1\n",
      "13          onehotencoder__neighbourhood_Astoria           1\n",
      "58  onehotencoder__neighbourhood_Windsor Terrace           0\n",
      "\n",
      "[65 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Initialize RFECV with the model\n",
    "rfe_cv = RFECV(LGBMRegressor(verbosity=-1, random_state=123, n_jobs=-1), cv=5, step=10)\n",
    "\n",
    "# Fit the model\n",
    "rfe_cv.fit(X_train_transformed, y_train)\n",
    "\n",
    "# If X_train is a DataFrame, we need to get the feature names\n",
    "# If you used a OneHotEncoder or similar, we get the feature names from the preprocessor\n",
    "if hasattr(preprocessor, 'get_feature_names_out'):  # If using OneHotEncoder or similar\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "else:\n",
    "    feature_names = X_train.columns  # If it's just scaling or simple transformations\n",
    "\n",
    "# Now apply the selected features mask to get the feature names\n",
    "selected_features = feature_names[rfe_cv.support_]\n",
    "\n",
    "# Get the fitted model from RFECV\n",
    "model = rfe_cv.estimator_\n",
    "\n",
    "# Get feature importances from the fitted model\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to show feature names with their corresponding importances\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': selected_features,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the feature importance\n",
    "print(importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Feature  Importance\n",
      "125     onehotencoder__neighbourhood_Theater District    0.134544\n",
      "120  onehotencoder__neighbourhood_Springfield Gardens    0.077014\n",
      "8            onehotencoder__minimum_nights_Short Stay    0.066137\n",
      "1                       pipeline-1__number_of_reviews    0.063016\n",
      "5             onehotencoder__minimum_nights_Long Stay    0.058757\n",
      "..                                                ...         ...\n",
      "127              onehotencoder__neighbourhood_Tremont    0.000509\n",
      "112             onehotencoder__neighbourhood_Rosebank    0.000469\n",
      "50           onehotencoder__neighbourhood_Eastchester    0.000383\n",
      "24           onehotencoder__neighbourhood_Boerum Hill    0.000311\n",
      "88               onehotencoder__neighbourhood_Midwood    0.000306\n",
      "\n",
      "[145 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Initialize RFECV with the model\n",
    "rfe_cv = RFECV(XGBRegressor(n_jobs=-1,random_state=123), cv=5, step=10)\n",
    "\n",
    "# Fit the model\n",
    "rfe_cv.fit(X_train_transformed, y_train)\n",
    "\n",
    "# If X_train is a DataFrame, we need to get the feature names\n",
    "# If you used a OneHotEncoder or similar, we get the feature names from the preprocessor\n",
    "if hasattr(preprocessor, 'get_feature_names_out'):  # If using OneHotEncoder or similar\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "else:\n",
    "    feature_names = X_train.columns  # If it's just scaling or simple transformations\n",
    "\n",
    "# Now apply the selected features mask to get the feature names\n",
    "selected_features = feature_names[rfe_cv.support_]\n",
    "\n",
    "# Get the fitted model from RFECV\n",
    "model = rfe_cv.estimator_\n",
    "\n",
    "# Get feature importances from the fitted model\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to show feature names with their corresponding importances\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': selected_features,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the feature importance\n",
    "print(importance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 10. Hyperparameter optimization <a name=\"10\"></a>\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Make some attempts to optimize hyperparameters for the models you've tried and summarize your results. In at least one case you should be optimizing multiple hyperparameters for a single model. You may use `sklearn`'s methods for hyperparameter optimization or fancier Bayesian optimization methods. \n",
    "  - [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)   \n",
    "  - [RandomizedSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "  - [scikit-optimize](https://github.com/scikit-optimize/scikit-optimize) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_10\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_estimators \n",
    " Number of boosting rounds\n",
    "\n",
    "learning_rate \n",
    " The learning rate of training\n",
    "controls how strongly each tree tries to correct the mistakes of the previous trees\n",
    "higher learning rate means each tree can make stronger corrections, which means more complex model\n",
    "\n",
    "max_depth \n",
    " max_depth of trees (similar to decision trees)\n",
    "\n",
    "scale_pos_weight \n",
    " Balancing of positive and negative weights [for class imbalance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hyperparameter_tuning_plot(preprocessor, X_train, y_train, type, parameters, train_color, cv_color, text_color):\n",
    "    \"\"\"\n",
    "    Make number_estimators vs score OR max_depth vs score plot for LGBMRegressor\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    preprocessor: sklearn transformer\n",
    "        The preprocessing pipeline to apply before the model\n",
    "    X_train: numpy.ndarray or DataFrame\n",
    "        The X part of the train set\n",
    "    y_train: numpy.ndarray or Series\n",
    "        The y part of the train set\n",
    "    type:\n",
    "        specify whether it is number_estimators vs score OR max_depth vs score plot\n",
    "    parameters: list of int\n",
    "        Values for `n_estimators` OR max_depth argument of LGBMRegressor\n",
    "    color1,2,3:\n",
    "        just for color customization\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Shows the number of estimators vs error rate plot\n",
    "    \"\"\"\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "\n",
    "    y_train_log = np.log1p(y_train)  # log(1 + y), avoids issues with 0\n",
    "\n",
    "    if type == \"n_estimators\":\n",
    "        for n in parameters:\n",
    "            model = make_pipeline(\n",
    "                preprocessor,\n",
    "                LGBMRegressor(\n",
    "                    verbosity=-1,\n",
    "                    random_state=123,\n",
    "                    n_jobs=-1,\n",
    "                    n_estimators=n\n",
    "                )\n",
    "            )\n",
    "            scores = cross_validate(\n",
    "                model,\n",
    "                X_train,\n",
    "                y_train_log,\n",
    "                return_train_score=True\n",
    "            )\n",
    "            train_scores.append(np.mean(scores[\"train_score\"]))\n",
    "            test_scores.append(np.mean(scores[\"test_score\"]))\n",
    "    elif type == \"max_depth\":\n",
    "        for m in parameters:\n",
    "            model = make_pipeline(\n",
    "                preprocessor,\n",
    "                LGBMRegressor(\n",
    "                    verbosity=-1,\n",
    "                    random_state=123,\n",
    "                    n_jobs=-1,\n",
    "                    max_depth=m\n",
    "                )\n",
    "            )\n",
    "            scores = cross_validate(\n",
    "                model,\n",
    "                X_train,\n",
    "                y_train_log,\n",
    "                return_train_score=True\n",
    "            )\n",
    "            train_scores.append(np.mean(scores[\"train_score\"]))\n",
    "            test_scores.append(np.mean(scores[\"test_score\"]))\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.semilogx(parameters, train_scores, label=\"train\", color=train_color, marker='o')\n",
    "    plt.semilogx(parameters, test_scores, label=\"cv\", color=cv_color, marker='o')\n",
    "\n",
    "    # Add text labels beside each point\n",
    "    for i, n in enumerate(parameters):\n",
    "        plt.text(n, test_scores[i], f\"{n}\", fontsize=12, ha='right', va='bottom', color=text_color)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(type)\n",
    "    plt.ylabel(\"score\")\n",
    "    plt.title(type + \" vs cv performance\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_hyperparameter_tuning_plot(preprocessor, X_train, y_train,\"n_estimators\",[1, 5, 10, 25, 50, 100, 200, 400], \"#96ddff\", \"#f6c5ef\", \"#dcbbf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_hyperparameter_tuning_plot(preprocessor, X_train, y_train,\"max_depth\",[1, 5, 10, 25, 50, 100, 200, 400],\"#ffcd94\",\"#9afcae\",\"#f19e9c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lgbm_tuned_first_time = make_pipeline(\n",
    "    preprocessor, LGBMRegressor(verbosity=-1, n_jobs=-1, random_state=123, n_estimators=50, max_depth=10)\n",
    ")\n",
    "results_dict[\"LGBM_tuned_first_time\"] = mean_std_cross_val_scores(\n",
    "    pipe_lgbm, X_train, y_train, cv=5, return_train_score=True\n",
    ")\n",
    "results_df = pd.DataFrame(results_dict).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# param_grid = {\"lgbmregressor__learning_rate\": [0.01, 0.1, 1],\n",
    "#                \"lgbmregressor__max_depth\": [100,200,300],\n",
    "#               }\n",
    "\n",
    "# search = RandomizedSearchCV(\n",
    "#     pipe_lgbm, param_grid, return_train_score=True, n_jobs=-1, verbose=0,random_state=123)\n",
    "\n",
    "# y_train_log = np.log1p(y_train)  # log(1 + y), avoids issues with 0\n",
    "\n",
    "# # Fit the search object\n",
    "# search.fit(X_train, y_train_log)\n",
    "\n",
    "# # Print the best hyperparameters and score\n",
    "# print(\"Best hyperparameter values: \", search.best_params_)\n",
    "# print(\"Best score: %0.3f\" % (search.best_score_))\n",
    "\n",
    "# # Convert the search results to a DataFrame and display\n",
    "# pd.DataFrame(search.cv_results_)[\n",
    "#     [\n",
    "#         \"mean_train_score\",\n",
    "#         \"mean_test_score\",\n",
    "#         \"param_lgbmregressor__learning_rate\",\n",
    "#         \"param_lgbmregressor__max_depth\",\n",
    "#         \"mean_fit_time\",\n",
    "#         \"rank_test_score\",\n",
    "#     ]\n",
    "# ].set_index(\"rank_test_score\").sort_index().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipe_lgbm_tuned_second_time = make_pipeline(\n",
    "#     preprocessor, LGBMRegressor(verbosity=-1,n_estimators=50, learning_rate=0.065, max_depth=10, random_state=123)\n",
    "# )\n",
    "# results_dict[\"LGBM_tuned_second_time\"] = mean_std_cross_val_scores(\n",
    "#     pipe_lgbm, X_train, y_train, cv=5, return_train_score=True\n",
    "# )\n",
    "# results_df = pd.DataFrame(results_dict).T\n",
    "# results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 11. Interpretation and feature importances <a name=\"1\"></a>\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Use the methods we saw in class (e.g., `shap`) (or any other methods of your choice) to examine the most important features of one of the non-linear models. \n",
    "2. Summarize your observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_11\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 12. Results on the test set <a name=\"12\"></a>\n",
    "<hr>\n",
    "\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Try your best performing model on the test data and report test scores. \n",
    "2. Do the test scores agree with the validation scores from before? To what extent do you trust your results? Do you think you've had issues with optimization bias? \n",
    "3. Take one or two test predictions and explain these individual predictions (e.g., with SHAP force plots).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_12\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 13. Summary of results <a name=\"13\"></a>\n",
    "<hr>\n",
    "rubric={points:12}\n",
    "\n",
    "Imagine that you want to present the summary of these results to your boss and co-workers. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create a table summarizing important results. \n",
    "2. Write concluding remarks.\n",
    "3. Discuss other ideas that you did not try but could potentially improve the performance/interpretability . \n",
    "3. Report your final test score along with the metric you used at the top of this notebook in the [Submission instructions section](#si)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_13\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 14. Your takeaway <a name=\"15\"></a>\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "What is your biggest takeaway from the supervised machine learning material we have learned so far? Please write thoughtful answers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_14\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLEASE READ BEFORE YOU SUBMIT:** \n",
    "\n",
    "When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from \"1\" will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "4. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission. \n",
    "5. Make sure that the plots and output are rendered properly in your submitted file. If the .ipynb file is too big and doesn't render on Gradescope, also upload a pdf or html in addition to the .ipynb so that the TAs can view your submission on Gradescope. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a tricky one but you did it! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-well-done.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc330",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
